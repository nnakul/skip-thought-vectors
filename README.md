## About
The project is inspired from the work of *Kiros et al.* published in the paper <a href = "https://arxiv.org/pdf/1506.06726.pdf"> *Skip-Thought Vectors* </a>. In this paper, the authors describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, an *encoder-decoder model* can be trained that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. The authors next introduce a simple *vocabulary expansion* method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. The end result of the implementation is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.

## Skip Thoughts Architecture
The architecture of the model is decribed in detail in *section 2.1* (*Inducing skip-thought vectors*) of the paper. On a high level, the model consists of a *GRU encoder* and two *thought-biased GRU decoders*, one for decoding the previous sentence and one for decoding the next sentence. The encoder takes as input a sequence of word embeddings representing the distinct words in the center sentence. The output of the encoder is a sequence of vectors, the last of which is the vector representation (*thought*) of the sentence.<br>
The decoders take the same input as the encoder. The *GRU* layers for the decoders are modified to account for the encoded vector representation of the center sentence. This is done by biasing the arguements of the *reset*, *update* and *new* gates in the layers by a scaled *thought* of the center sentence, as shown below. <br>
The output of both the decoders is a sequence of vectors that represent the distinct words in the sentence adjacent to the center sentence (previous or next). As opposed to a dense layer followed by a plain softmax layer to classify these representations as words in the vocabulary, *negative-sampling* is used to get high-quality thoughts in a much shorter training time. *Negative sampling* was popularized by *Mikolov et al.* in their paper <a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf"> *Distributed Representations of Words and Phrases and their Compositionality* </a>, the proposed approach of which is implemented in <a href = "https://github.com/nnakul/word-embeddings"> this </a> project.

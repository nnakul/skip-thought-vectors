## About
The project is inspired from the work of *Kiros et al.* published in the paper <a href = "https://arxiv.org/pdf/1506.06726.pdf"> *Skip-Thought Vectors* </a>. In this paper, the authors describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, an *encoder-decoder model* can be trained that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. The authors next introduce a simple *vocabulary expansion* method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. The end result of the implementation is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.
